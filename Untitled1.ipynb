{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPkncn8w0UDammVOMg12L88",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rebecamctf/DIO_CURSOS/blob/main/Untitled1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2f_IQUKAZgVU"
      },
      "outputs": [],
      "source": [
        "# importando as libs\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import gym\n",
        "import random\n",
        "from IPython.display import clear_output\n",
        "from time import sleep"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#A primeira linha do código carrega o ambiente que vamos usar, no nosso caso “Taxi-v3”. Ele fica salvo na variável env.\n",
        "\n",
        "#Com o método .render(), renderizamos o estado atual do ambiente.\n",
        "\n",
        "\n",
        "env = gym.make(\"Taxi-v3\").env\n",
        "env.render()"
      ],
      "metadata": {
        "id": "aHgDoLhUZqRr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Outros métodos também muito importantes são:\n",
        "\n",
        "env.reset: Reseta o ambiente e retorna um estado inicial aleatório.\n",
        "env.step (action): Executa uma ação (passo). Retorna:\n",
        "* observation: observações do ambiente.\n",
        "\n",
        "* reward: recompensa recebida após a ação tomada.\n",
        "\n",
        "* done: indica se foi o fim de um episódio. Acontece quando um passageiro embarca no táxi e desembarca no local correto.\n",
        "\n",
        "* info: informação adicional para debbuging. O agente não tem acesso a elas.\n",
        "\n",
        "Podemos também conferir as dimensões do espaço de ações e espaço de estados, como segue:"
      ],
      "metadata": {
        "id": "uUjg1AsTaI8a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "nzZ9WlWgaI15"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dimensões dos espaços\n",
        "#Espaço de ações e espaço de estados, respectivamente:\n",
        "\n",
        "\n",
        "print(env.action_space)\n",
        "print(env.observation_space)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-aUw-BrZ-LX",
        "outputId": "b2f2b494-d15d-4544-bd3f-b5343bfd1a44"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Discrete(6)\n",
            "Discrete(500)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "O que condiz com o que calculamos anteriormente.\n",
        "\n",
        "Q-learning\n",
        "Vamos agora recordar das recompensas para cada ação. Elas já estão pré-definidas na biblioteca da seguinte maneira:\n",
        "\n",
        "+20 para um desembarque correto.\n",
        "-10 para um embarque ou desembarque incorreto.\n",
        "-1 para ações que não sejam as duas anteriores.\n",
        "A recompensa de -1 é importante pois garante que o carro evite paredes (caso fique tentando entrar em uma, irá receber -1 indefinidamente) e que o agente busque se otimizar de forma a performar o menor caminho possível até o ponto de desembarque.\n",
        "\n",
        "Como dito antes, caso não saiba o que é Q-learning, leia o nosso post anterior. Precisamos determinar os q-values para cada tupla (estado, ação), que irão orientar a política do nosso agente, ou seja 500 x 6 = 3.000 valores. A partir da equação de Bellman, determinamos os q-values como segue:\n",
        "\n",
        "\n",
        "Onde:\n",
        "\n",
        "α é a taxa de aprendizado (0 < α ≤ 1), que representa o quanto nosso q-value está sendo atualizado em cada iteração.\n",
        "γ é o fator de desconto (0 < γ ≤ 1), que representa a importância que damos para recompensas futuras. γ próximo de 1 favorece as recompensas mais distantes enquanto gama perto de 0 praticamente às desconsidera.\n",
        "O que estamos fazendo é atualizar o q-value da ação tomada no estado atual. Tomamos o valor antigo com um peso de (1- α) e adicionamos o novo valor aprendido, que é a combinação da recompensa por realizar a ação tomada no estado atual, com recompensa máxima, descontada (γ), do próximo estado que estaremos assim que tomarmos a ação.\n",
        "\n",
        "Essa política fará com que o agente tome a rota que fornecerá a maior soma de recompensas possível.\n",
        "\n",
        "Tabela-Q\n",
        "\n",
        "Para guardar esses valores vamos criar uma tabela Estados x Ações:\n",
        "\n",
        "\n",
        "Iniciamos a tabela preenchida com zeros e a cada iteração atualizamos os valores.\n",
        "\n",
        "Passo-a-passo, fica:\n",
        "\n",
        "Inicializar a tabela-Q preenchida com zeros.\n",
        "Para as ações seguintes, há uma chance do agente tomá-las de forma aleatória ou seguir a política da tabela Estados x Ações.\n",
        "Após episódios suficientes, é esperado que o agente atinja uma política ótima e a tabela Estados x Ações esteja com os valores o mais próximo do ideal possível."
      ],
      "metadata": {
        "id": "HOJmvfzfaQRr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Implementação em código\n",
        "\n",
        "#Inicializando a tabela-q\n",
        "#Note aqui a razão de precisarmos ter um número finito de estados possíveis. Caso contrário não seria possível mapeá-los em uma tabela\n",
        "\n",
        "\n",
        "\n",
        "tabela_q = np.zeros([env.observation_space.n, env.action_space.n]) #inician"
      ],
      "metadata": {
        "id": "SoXqbobAaV10"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Treinando o algoritmo\n",
        "#Aplicando tudo aquilo que acabamos de explicar, segue:\n",
        "\n",
        "\n",
        "\n",
        "#treinando o algoritmo\n",
        "\n",
        "#aqui não existem valores \"certos\" ou \"errados\", decidimos por tentativa e erro aqueles que otimizaram o treinamento do nosso agente\n",
        "alpha = 0.1\n",
        "gamma = 0.6\n",
        "epsilon = 0.1 #determina a chance do agente tomar uma ação aleatória, nesse caso a chance é de 10%\n",
        "\n",
        "for i in range(1, 50001):\n",
        "    estado = env.reset()\n",
        "\n",
        "    epochs, penalidades, recompensa = 0, 0, 0 #epochs é cada episódio\n",
        "    terminado = False\n",
        "    \n",
        "    while not terminado:\n",
        "        if random.uniform(0, 1) < epsilon: #decidindo se será tomado uma ação aleatória ou se seguirá a política da tabela-q\n",
        "            acao = env.action_space.sample() \n",
        "        else:\n",
        "            acao = np.argmax(tabela_q[estado]) \n",
        "\n",
        "        proximo_estado, recompensa, terminado, info = env.step(acao) \n",
        "        \n",
        "        valor_antigo = tabela_q[estado, acao]\n",
        "        proximo_max = np.max(tabela_q[proximo_estado])\n",
        "        \n",
        "        valor_novo = (1 - alpha) * valor_antigo + alpha * (recompensa + gamma * proximo_max) #atualizando o valor de q a partir da equação de Bellman\n",
        "        tabela_q[estado, acao] = valor_novo #colocando este valor na tabela-q\n",
        "\n",
        "        if recompensa == -10: #contabilizando os embarques/desembarques errados\n",
        "            penalidades += 1\n",
        "\n",
        "        estado = proximo_estado\n",
        "        epochs += 1\n",
        "        \n",
        "        clear_output(wait=True) #caso não queira ver o aprendizado comentar as 3 linhas seguintes, essa incluso\n",
        "        env.render()\n",
        "        sleep(.25)  #aumentar se quiser ver melhor o aprendizado (recomendado: .25)\n",
        "        \n",
        "    if i % 100 == 0:\n",
        "        clear_output(wait=True)\n",
        "        print(f\"Episódios: {i}\")\n",
        "        #sleep(1)\n",
        "\n",
        "print(\"Treinamento terminado.\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YwpfNilPawQH",
        "outputId": "40bbe118-b179-4101-8c34-b6351f28a0e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+\n",
            "|\u001b[34;1mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| :\u001b[43m \u001b[0m|\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (South)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Treinando o algoritmo\n",
        "#Aplicando tudo aquilo que acabamos de explicar, segue:\n",
        "\n",
        "#treinando o algoritmo\n",
        "\n",
        "#aqui não existem valores \"certos\" ou \"errados\", decidimos por tentativa e erro aqueles que otimizaram o treinamento do nosso agente\n",
        "alpha = 0.1\n",
        "gamma = 0.6\n",
        "epsilon = 0.1 #determina a chance do agente tomar uma ação aleatória, nesse caso a chance é de 10%\n",
        "\n",
        "for i in range(1, 50001):\n",
        "    estado = env.reset()\n",
        "\n",
        "    epochs, penalidades, recompensa = 0, 0, 0 #epochs é cada episódio\n",
        "    terminado = False\n",
        "    \n",
        "    while not terminado:\n",
        "        if random.uniform(0, 1) < epsilon: #decidindo se será tomado uma ação aleatória ou se seguirá a política da tabela-q\n",
        "            acao = env.action_space.sample() \n",
        "        else:\n",
        "            acao = np.argmax(tabela_q[estado]) \n",
        "\n",
        "        proximo_estado, recompensa, terminado, info = env.step(acao) \n",
        "        \n",
        "        valor_antigo = tabela_q[estado, acao]\n",
        "        proximo_max = np.max(tabela_q[proximo_estado])\n",
        "        \n",
        "        valor_novo = (1 - alpha) * valor_antigo + alpha * (recompensa + gamma * proximo_max) #atualizando o valor de q a partir da equação de Bellman\n",
        "        tabela_q[estado, acao] = valor_novo #colocando este valor na tabela-q\n",
        "\n",
        "        if recompensa == -10: #contabilizando os embarques/desembarques errados\n",
        "            penalidades += 1\n",
        "\n",
        "        estado = proximo_estado\n",
        "        epochs += 1\n",
        "        \n",
        "        clear_output(wait=True) #caso não queira ver o aprendizado comentar as 3 linhas seguintes, essa incluso\n",
        "        env.render()\n",
        "        sleep(.25)  #aumentar se quiser ver melhor o aprendizado (recomendado: .25)\n",
        "        \n",
        "    if i % 100 == 0:\n",
        "        clear_output(wait=True)\n",
        "        print(f\"Episódios: {i}\")\n",
        "        #sleep(1)\n",
        "\n",
        "print(\"Treinamento terminado.\\n\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vDK2hzzDa2zv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Testando o agente\n",
        "#com a tabela-q atualizada, vejamos como o nosso agente se sai:\n",
        "\n",
        "\n",
        "#testando o algoritmo\n",
        "epochs_totais, penalidades_totais = 0, 0\n",
        "episodios = 100\n",
        "\n",
        "for _ in range(episodios):\n",
        "    estado = env.reset()\n",
        "    epochs, penalidades, recompensa = 0, 0, 0\n",
        "    \n",
        "    terminado = False\n",
        "    \n",
        "    while not terminado:\n",
        "        acao = np.argmax(tabela_q[estado])\n",
        "        estado, recompensa, terminado, info = env.step(acao)\n",
        "\n",
        "        if recompensa == -10:\n",
        "            penalidades += 1\n",
        "\n",
        "        epochs += 1\n",
        "        \n",
        "        clear_output(wait=True)\n",
        "        env.render()\n",
        "        sleep(.25)\n",
        "\n",
        "    penalidades_totais += penalidades\n",
        "    epochs_totais += epochs\n",
        "\n",
        "print(f\"Resutados depois de {episodios} episodios:\")\n",
        "print(f\"Média de passos por episódio: {epochs_totais / episodios}\")\n",
        "print(f\"Média de penalidades por episódio: {penalidades_totais / episodios}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "C1Lt0FhybM_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dos 100 episódios testados, nosso agente não recebeu nenhuma penalidade, o que é um ótimo parâmetro para medirmos o sucesso de seu treinamento. Em relação a média de passos, também obtemos um bom valor (metade das casas do tabuleiro percorridas) tendo em vista a existência dos obstáculos.\n",
        "\n",
        "Conclusão\n",
        "Neste post apresentamos a Gym, uma ferramenta poderosa que facilita muito a vida de quem quer iniciar os estudos em Reinforcement Learning. Apresentamos aqui seus diferentes tipos de ambientes e suas principais funcionalidades fazendo a aplicação em um exemplo prático.\n",
        "\n",
        "Além de ser essa ótima ferramenta para iniciantes, ela também é de grande valor no meio acadêmico. Isso porque, ao normalizar o espaço de trabalho que as pesquisas são desenvolvidas, ou seja estudos diferentes desenvolvidos no mesmo ambiente, torna-se mais fácil e de maior confiabilidade se comparar os resultados obtidos.\n",
        "\n",
        "Trabalhamos para implementar o Q-learning, um dos algoritmos mais simples de RL, ideal para os propósitos didáticos do post. Sua aplicação se limita a cenários com um número de estados finito e suficientemente pequeno. Caso esse valor se torne muito grande ou indefinido, devemos procurar outros métodos mais complexos de implementação de RL.\n",
        "\n",
        "Atualmente, para resolver os problemas mais abstratos costuma-se usar as Deep Neural Networks que recebem na camada de entradas informações do estado e ações para poder aproximar um valor bom o suficiente para o q-value, mesmo sem levar em consideração todos os estados posteriores possíveis (até porque podem existir infinitos), aprendendo a retornar as ações corretas após treinamento suficiente.\n",
        "\n",
        "Esse é o tipo de abordagem que se usa para treinar braços robóticos, jogos sem finais definidos como Flappy Bird, decidir preços praticados por uma empresa de acordo com as flutuações do mercado, gerenciamento de estoques ou realizar previsões no mercado financeiro. Note, portanto, que apesar da maior complexidade, a ideia central é a mesma apresentada em Q-learning.\n",
        "\n",
        "Por fim, vamos voltar nossa atenção aos hiperparâmetros(α, γ e ε). Para decidi-los, tomamos uma abordagem de tentativa e erro. O ideal seria os ajustarmos de forma a diminuí-los com o passar do tempo, porém, por termos um espaço de estados tão pequeno, conseguimos atingir uma política ótima com valores estáticos. A justificativa para a diminuição de cada um é:\n",
        "\n",
        "α: quanto mais próximo do fim do treino, com uma base de conhecimento sobre o ambiente já consolidada, uma alta taxa de aprendizado torna-se menos relevante.\n",
        "γ: Quanto mais próximo do final do treinamento, o comportamento do agente para os finais dos episódios já estão bem definido e, se estiver aprendendo de forma correta, estará de acordo com o esperado. É interessante então melhorar o comportamento para as situações mais iniciais.\n",
        "ε: Quanto mais atualizada a tabela-q, menos o agente precisa explorar ações aleatórias.\n",
        "Para os aficionados em RL que chegaram até aqui, nossos mais profundos agradecimentos. Não se esqueçam de conferir nossas redes: Medium, Facebook, Instagram e LinkedIn. Caso tenha interesse, você pode encontrar o notebook completo do post aqui.\n",
        "\n",
        "Até a próxima!"
      ],
      "metadata": {
        "id": "Jp3YKIdHbbkt"
      }
    }
  ]
}